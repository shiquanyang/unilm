{
  "ablationG": 0,
  "ablationH": 0,
  "adam_epsilon": 1e-08,
  "addName": "",
  "batch": 8,
  "cache_dir": "/Users/shiquan/PycharmProjects/unilm/s2s-ft/tmp/dependencies",
  "cached_train_features_file": null,
  "clip": 10,
  "config_name": null,
  "convkernelsize": 3,
  "dataset": null,
  "decoder": null,
  "do_lower_case": true,
  "drop": 0.2,
  "earlyStop": "BLEU",
  "evalp": 1,
  "fp16": false,
  "fp16_opt_level": "O1",
  "genSample": 0,
  "gradient_accumulation_steps": 1,
  "hidden": 128,
  "inchannels": 1024,
  "keep_prob": 0.1,
  "label_smoothing": 0.1,
  "layer": 1,
  "learn": 0.001,
  "learning_rate": 7e-05,
  "limit": -10000,
  "local_rank": -1,
  "log_dir": null,
  "logging_steps": 500,
  "max_grad_norm": 1.0,
  "max_source_seq_length": 60,
  "max_target_seq_length": 40,
  "model_name_or_path": "unilm1.2-base-uncased",
  "model_type": "unilm",
  "no_cuda": false,
  "num_training_epochs": 10,
  "num_training_steps": 32000,
  "num_warmup_steps": 500,
  "outchannels": 256,
  "output_dir": "/Users/shiquan/PycharmProjects/MultiModalKB/s2s-ft/tmp/finetuned_models",
  "path": null,
  "per_gpu_train_batch_size": 8,
  "poolkernelsize": 3,
  "random_prob": 0.1,
  "record": 0,
  "sample": null,
  "save_steps": 1500,
  "seed": 42,
  "server_ip": "",
  "server_port": "",
  "task": "",
  "teacher_forcing_ratio": 0.5,
  "tokenizer_name": null,
  "train_file": "/Users/shiquan/PycharmProjects/unilm/s2s-ft/train.json",
  "unk_mask": 1,
  "weight_decay": 0.01
}